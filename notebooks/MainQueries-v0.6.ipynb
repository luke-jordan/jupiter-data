{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery, storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dabl\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, auc\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FETCH DATA AND LABEL IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_and_upload_df(dataframe, file_prefix, bucket_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    \n",
    "    file_name = f\"{file_prefix}_{datetime.today().strftime('%Y_%m_%dT%H:%M:%S')}.csv\"\n",
    "    dataframe.to_csv(file_name, index=False)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    blob.upload_from_filename(f\"./{file_name}\")\n",
    "\n",
    "    print(f\"File {file_name} uploaded to {bucket_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_monthly_save_aggregates():\n",
    "    sql = \"\"\"\n",
    "        select EXTRACT(MONTH from TIMESTAMP_MILLIS(time_transaction_occurred)) as save_month, unit, \n",
    "            sum(amount) as sum, avg(amount) as average, count(*) as count from ops.user_behaviour \n",
    "            where transaction_type = 'SAVING_EVENT' group by save_month, unit order by save_month desc;\n",
    "    \"\"\"\n",
    "    \n",
    "    df = client.query(sql).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_boosts_with_saves():\n",
    "    sql = \"\"\"\n",
    "    with boost_offers as (\n",
    "            select *, TIMESTAMP_MILLIS(created_at) as creation_timestamp \n",
    "            from ops.all_user_events \n",
    "            where event_type like 'BOOST_CREATED%'\n",
    "\n",
    "    ), save_events as (\n",
    "            select *, TIMESTAMP_MILLIS(created_at) as creation_timestamp \n",
    "            from ops.all_user_events \n",
    "            where event_type = 'SAVING_PAYMENT_SUCCESSFUL'\n",
    "    )\n",
    "    select boost_offers.user_id, boost_offers.event_type, boost_offers.context, \n",
    "        boost_offers.creation_timestamp as boost_creation_time, save_events.creation_timestamp as save_completion_time,  \n",
    "        TIMESTAMP_DIFF(save_events.creation_timestamp, boost_offers.creation_timestamp, HOUR) as time_from_boost_to_save\n",
    "    from boost_offers left join save_events on boost_offers.user_id = save_events.user_id\n",
    "    \"\"\"\n",
    "    \n",
    "    df = client.query(sql).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_boosts_with_prior_redemptions():\n",
    "    sql = \"\"\"\n",
    "    with boost_offers as (\n",
    "            select *, TIMESTAMP_MILLIS(created_at) as creation_timestamp \n",
    "            from ops.all_user_events \n",
    "            where event_type like 'BOOST_CREATED%'\n",
    "\n",
    "    ), boost_redemptions as (\n",
    "            select *, TIMESTAMP_MILLIS(created_at) as creation_timestamp \n",
    "            from ops.all_user_events \n",
    "            where event_type = 'BOOST_REDEEMED'\n",
    "    )\n",
    "    select boost_offers.user_id, boost_offers.event_type, boost_offers.context, \n",
    "        boost_offers.creation_timestamp as boost_creation_time, boost_redemptions.creation_timestamp as boost_redemption_time,  \n",
    "        TIMESTAMP_DIFF(boost_redemptions.creation_timestamp, boost_offers.creation_timestamp, HOUR) as time_from_boost_to_last_redeem\n",
    "    from boost_offers left join boost_redemptions on boost_offers.user_id = boost_redemptions.user_id\n",
    "        where TIMESTAMP_DIFF(boost_redemptions.creation_timestamp, boost_offers.creation_timestamp, HOUR) < 0 or\n",
    "        TIMESTAMP_DIFF(boost_redemptions.creation_timestamp, boost_offers.creation_timestamp, HOUR) is null\n",
    "    \"\"\"\n",
    "\n",
    "    df = client.query(sql).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_context_and_set_boost_id(df):\n",
    "    # extract a bunch of context from the boosts    \n",
    "    df[\"parsed_context\"] = df.context.apply(json.loads)\n",
    "    df[\"boost_id\"] = df[\"parsed_context\"].apply(lambda context: context[\"boostId\"])\n",
    "    # and this functions as our index     \n",
    "    df[\"boost_user_id\"] = df[\"boost_id\"] + \"::\" + df[\"user_id\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prior_save_counts(prior_save_counts):\n",
    "    print('Past rows: ', prior_save_counts.shape)\n",
    "    prior_save_counts[\"boost_prior_saves\"] = prior_save_counts.groupby('boost_user_id').transform('count')[\"save_completion_time\"]\n",
    "    prior_save_counts = prior_save_counts[[\"boost_user_id\", \"boost_prior_saves\"]]\n",
    "    prior_save_counts = prior_save_counts.groupby(\"boost_user_id\").first() # no need for a sort\n",
    "    return prior_save_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_since_latest_save(prior_save_df):\n",
    "    with_latest_save = prior_save_df.sort_values(\"save_completion_time\").groupby(\"boost_user_id\", as_index = False).last()\n",
    "    with_latest_save[\"days_since_latest_save\"] = abs(with_latest_save[\"time_from_boost_to_save\"] / 24)\n",
    "    with_latest_save = with_latest_save[[\"boost_user_id\", \"days_since_latest_save\"]]\n",
    "    return with_latest_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_since_first_save(prior_save_df):\n",
    "    # for some reason if index, causes issues here\n",
    "    with_earliest_save = prior_save_df.sort_values(\"save_completion_time\").groupby(\"boost_user_id\", as_index = False).first()\n",
    "    with_earliest_save[\"days_since_first_save\"] = abs(with_earliest_save[\"time_from_boost_to_save\"] / 24)\n",
    "    with_earliest_save = with_earliest_save[[\"boost_user_id\", \"days_since_first_save\"]]\n",
    "    return with_earliest_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prior_redemption(df):\n",
    "    df = parse_context_and_set_boost_id(df)\n",
    "    print('Priors, length: ', df.shape)\n",
    "    adjusted_df = df.sort_values(\"time_from_boost_to_last_redeem\").groupby(\"boost_user_id\", as_index=False).last()\n",
    "    adjusted_df[\"has_prior_redeemed\"] = adjusted_df.boost_redemption_time.notna()\n",
    "    return adjusted_df[[\"boost_user_id\", \"has_prior_redeemed\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_and_construct_labels(boosts_with_saves, boosts_with_prior_redeemed):\n",
    "    unit_convertors = { 'WHOLE_CURRENCY': 1, 'WHOLE_CENT': 100, 'HUNDREDTH_CENT': 10000 }\n",
    "    \n",
    "    df = boosts_with_saves\n",
    "    print('Starting count: ', df.shape)\n",
    "    \n",
    "    df['user_id_count'] = boosts_with_saves.groupby(['user_id'])['boost_creation_time'].transform('count')\n",
    "    \n",
    "    # we remove the top 2, because they are team members often testing, so distort\n",
    "    outlier_user_ids = df['user_id'].value_counts()[:2].index.tolist()\n",
    "    # probably a better panda-ninja way to do this but not worth it right now\n",
    "    for user_id in outlier_user_ids:\n",
    "        df = df[df.user_id != user_id]\n",
    "        \n",
    "    print('With outlier top users stripped: ', df.shape)\n",
    "    \n",
    "    df = parse_context_and_set_boost_id(df)\n",
    "    \n",
    "    # here we have our label\n",
    "    df[\"is_save_within_day\"] = df[\"time_from_boost_to_save\"] < 24\n",
    "    \n",
    "    df[\"boost_amount_whole_currency\"] = df[\"parsed_context\"].apply(\n",
    "        lambda context: int(context[\"boostAmount\"]) / unit_convertors[context[\"boostUnit\"]])\n",
    "    \n",
    "    df[\"boost_type\"] = df[\"parsed_context\"].apply(lambda context: context[\"boostType\"])\n",
    "    df[\"boost_category\"] = df[\"parsed_context\"].apply(lambda context: context[\"boostCategory\"])\n",
    "    df[\"boost_type_category\"] = df[\"boost_type\"] + \"::\" + df[\"boost_category\"]\n",
    "    \n",
    "    print('Categories: ', df[\"boost_type_category\"].unique())\n",
    "    \n",
    "    df[\"day_of_month\"] = df[\"boost_creation_time\"].dt.day\n",
    "    df[\"hour_of_day\"] = df[\"boost_creation_time\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"boost_creation_time\"].dt.dayofweek\n",
    "    \n",
    "    # then we construct our future and past masks, calculate prior saves, and find next save\n",
    "    prior_save_mask = df[\"time_from_boost_to_save\"] < 0\n",
    "    future_save_mask = df[\"time_from_boost_to_save\"] > 0\n",
    "        \n",
    "    # likely a way to do these more simply, but for now doing groups & sorts differently    \n",
    "    prior_save_counts = extract_prior_save_counts(df[prior_save_mask].copy())\n",
    "    days_since_latest_save = extract_time_since_latest_save(df[prior_save_mask].copy())\n",
    "    days_since_first_save = extract_time_since_first_save(df[prior_save_mask].copy())\n",
    "    \n",
    "    # then we discard the past -- NOTE : it looks like doing this, because of the sort, discards any\n",
    "    # boost type categories that do not have more than 2 redemptions -- should fix soon    \n",
    "    with_future_saves = df[future_save_mask].copy()\n",
    "    print('And with future saves: ', with_future_saves['boost_type_category'].unique())\n",
    "    \n",
    "    with_next_save = with_future_saves.sort_values(\"save_completion_time\").groupby(\"boost_user_id\").first()\n",
    "    print('Unique types, with next save: ', with_next_save['boost_type_category'].unique())\n",
    "    \n",
    "    print('Now with just future saves crossed: ', with_future_saves.shape, ' and next save only: ', with_next_save.shape)\n",
    "    \n",
    "    with_prior_redemption = extract_prior_redemption(boosts_with_prior_redeemed)\n",
    "    \n",
    "    # and finally we strip out the surplus boost-save pairs (by retaining only the opening)\n",
    "    # at the moment an inner join, but we may want to turn this into joining from those with saves\n",
    "    final_df = pd.merge(with_next_save, prior_save_counts, on='boost_user_id')\n",
    "    print('Categories, final DF: ', final_df[\"boost_type_category\"].unique())\n",
    "\n",
    "    final_df = pd.merge(final_df, days_since_latest_save, on='boost_user_id')\n",
    "    final_df = pd.merge(final_df, days_since_first_save, on='boost_user_id')\n",
    "    print(\"And finally, stripped to just one per: \", final_df.shape)\n",
    "    \n",
    "    final_df = pd.merge(final_df, with_prior_redemption, on='boost_user_id')\n",
    "    print(\"And now with boolean on prior redemption: \", final_df.shape)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_withdraw_then_save(df):\n",
    "    withdrawals_with_next_save = \"\"\"\n",
    "        with withdrawal_events as (\n",
    "          select user_id, event_type, timestamp_millis(time_transaction_occurred) as withdrawal_time, context\n",
    "          from ops.all_user_events where event_type = 'ADMIN_SETTLED_WITHDRAWAL'\n",
    "        ),\n",
    "        save_events as (\n",
    "          select user_id, transaction_type, timestamp_millis(time_transaction_occurred) as save_time, amount, unit\n",
    "          from ops.user_behaviour where transaction_type = 'SAVING_EVENT'\n",
    "        )\n",
    "        select withdrawal_events.user_id, withdrawal_events.withdrawal_time as withdrawal_time, \n",
    "        min(save_events.save_time) as next_save_time, count(save_events.save_time > withdrawal_time) as subsequent_save_count,\n",
    "        from withdrawal_events left join save_events on withdrawal_events.user_id = save_events.user_id\n",
    "        where (save_events.save_time > withdrawal_time) \n",
    "        group by user_id, withdrawal_events.withdrawal_time\n",
    "    \"\"\"\n",
    "\n",
    "    wdf = client.query(withdrawals_with_next_save).to_dataframe()\n",
    "    slimmed_df = wdf[['user_id', 'subsequent_save_count']]\n",
    "    slimmed_df = slimmed_df.groupby('user_id').sum()\n",
    "    with_withdrawal = df.merge(slimmed_df, how='left', on='user_id')\n",
    "    with_withdrawal['has_withdrawn_and_saved'] = with_withdrawal['subsequent_save_count'] > 0\n",
    "    return with_withdrawal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosts_with_saves = obtain_boosts_with_saves()\n",
    "boosts_with_redeems = obtain_boosts_with_prior_redemptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_up_and_construct_labels(boosts_with_saves, boosts_with_redeems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_withdraw_then_save(data).rename(columns={'subsequent_save_count': 'saves_after_withdraw'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.has_prior_redeemed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_save_within_day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_store['n'] = len(data)\n",
    "result_store['n_positive'] = data.is_save_within_day.value_counts()[True]\n",
    "print(result_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data):\n",
    "    features_of_interest = [\n",
    "        \"boost_amount_whole_currency\", \n",
    "        \"day_of_month\", \n",
    "        \"boost_prior_saves\",\n",
    "        \"boost_type_category\",\n",
    "        \"days_since_latest_save\",\n",
    "        \"has_prior_redeemed\",\n",
    "        \"days_since_first_save\",\n",
    "        \"day_of_week\",\n",
    "        \"saves_after_withdraw\",\n",
    "        \"has_withdrawn_and_saved\",\n",
    "        \"is_save_within_day\"\n",
    "    ]\n",
    "    stripped_df = data[features_of_interest]\n",
    "    return stripped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_frame = feature_extraction(data)\n",
    "feature_frame.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_and_upload_df(dataframe, file_prefix='boost_save_inducement', bucket_name='prod_boost_ml_datasets'):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    file_name = f\"{file_prefix}_{datetime.today().strftime('%Y_%m_%dT%H:%M:%S')}.csv\"\n",
    "    dataframe.to_csv(file_name, index=False)\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    blob.upload_from_filename(f\"./{file_name}\")\n",
    "\n",
    "    print(f\"File {file_name} uploaded to {bucket_name}.\")\n",
    "    \n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_dataset = export_and_upload_df(feature_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE DABL AUTO TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_frame.head()\n",
    "dabl_data = dabl.clean(feature_frame)\n",
    "dabl_data.dtypes\n",
    "dabl.plot(dabl_data, target_col='is_save_within_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dabl_data.drop(\"is_save_within_day\", axis=1)\n",
    "Y = dabl_data.is_save_within_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment this is making things worse, so\n",
    "preprocessor = dabl.EasyPreprocessor()\n",
    "X_trans = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = dabl.SimpleClassifier(random_state=0)\n",
    "fc.fit(X_trans, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y, fc.predict(X), pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARTING GOOGLE AUTOML SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'prod_boost_ml_datasets'\n",
    "dataset_path = f'gs://{bucket_name}/{latest_dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import automl_v1beta1 as automl\n",
    "client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=./google_service_account.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up client\n",
    "client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}\n",
    "\n",
    "automl_client = automl.TablesClient(project='jupiter-production-258809', region='eu', client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with the given display name (gcloud currently only allows 32 chars, and no :, so)\n",
    "import time\n",
    "timestamp = int(time.time()*1000.0)\n",
    "dataset_display_name = f\"boost_select_{timestamp}\"\n",
    "print('Dataset display name: ', dataset_display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = automl_client.create_dataset(dataset_display_name)\n",
    "\n",
    "# Display the dataset information.\n",
    "print(\"Dataset name: {}\".format(dataset.name))\n",
    "print(\"Dataset id: {}\".format(dataset.name.split(\"/\")[-1]))\n",
    "print(\"Dataset display name: {}\".format(dataset.display_name))\n",
    "print(\"Dataset metadata:\")\n",
    "print(\"\\t{}\".format(dataset.tables_dataset_metadata))\n",
    "print(\"Dataset example count: {}\".format(dataset.example_count))\n",
    "print(\"Dataset create time:\")\n",
    "print(\"\\tseconds: {}\".format(dataset.create_time.seconds))\n",
    "print(\"\\tnanos: {}\".format(dataset.create_time.nanos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dataset_path\n",
    "response = None\n",
    "\n",
    "if path.startswith(\"bq\"):\n",
    "    response = automl_client.import_data(\n",
    "        dataset_display_name=dataset_display_name, bigquery_input_uri=path\n",
    "    )\n",
    "else:\n",
    "    # Get the multiple Google Cloud Storage URIs.\n",
    "    input_uris = path.split(\",\")\n",
    "    response = automl_client.import_data(\n",
    "        dataset_display_name=dataset.display_name,\n",
    "        gcs_input_uris=input_uris,\n",
    "    )\n",
    "\n",
    "print(\"Processing import...\")\n",
    "# synchronous check of operation status.\n",
    "print(\"Data imported. {}\".format(response.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_spec_display_name = 'is_save_within_day' #@param {type:'string'}\n",
    "\n",
    "update_dataset_response = automl_client.set_target_column(\n",
    "    dataset=dataset,\n",
    "    column_spec_display_name=column_spec_display_name,\n",
    ")\n",
    "update_dataset_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'jupiter-production-258809'\n",
    "compute_region = 'eu'\n",
    "dataset_display_name = dataset_display_name\n",
    "model_display_name = dataset_display_name\n",
    "train_budget_milli_node_hours = 5000\n",
    "# include_column_spec_names = 'INCLUDE_COLUMN_SPEC_NAMES_HERE'\n",
    "#    or None if unspecified\n",
    "# exclude_column_spec_names = 'EXCLUDE_COLUMN_SPEC_NAMES_HERE'\n",
    "#    or None if unspecified\n",
    "\n",
    "response = automl_client.create_model(\n",
    "    model_display_name,\n",
    "    train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    dataset_display_name=dataset_display_name,\n",
    "    include_column_spec_names=None,\n",
    "    exclude_column_spec_names=None,\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "print(\"Training operation name: {}\".format(response.operation.name))\n",
    "print(\"Training completed: {}\".format(response.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARTING SVM SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_all_one_hots(df):\n",
    "    all_boost_type_categories = [\n",
    "        'GAME::CHASE_ARROW', \n",
    "        'GAME::DESTROY_IMAGE',\n",
    "        'GAME::TAP_SCREEN', \n",
    "        'SIMPLE::ROUND_UP',\n",
    "        'SIMPLE::SIMPLE_SAVE', \n",
    "        'SIMPLE::TIME_LIMITED',\n",
    "        'SIMPLE::TARGET_BALANCE',\n",
    "        'SOCIAL::FRIENDS_ADDED',\n",
    "        'SOCIAL::NUMBER_FRIENDS'\n",
    "    ]\n",
    "    \n",
    "    assignment_args = {}\n",
    "    for category in all_boost_type_categories:\n",
    "        column_name = f'boost_type_category_{category}'\n",
    "        if column_name not in df:\n",
    "            assignment_args[column_name] = 0\n",
    "    \n",
    "    return df.assign(**assignment_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = feature_frame[[\"boost_amount_whole_currency\", \"day_of_month\", \"day_of_week\", \"boost_prior_saves\", \"days_since_latest_save\", \"has_withdrawn_and_saved\", \"boost_type_category\"]]\n",
    "# will one hot encode day of week when more data so less sparse\n",
    "X_encoded = pd.get_dummies(X_small, prefix_sep=\"_\", columns=[\"boost_type_category\", \"day_of_week\"]) \n",
    "X_encoded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = ensure_all_one_hots(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_encoded, data.is_save_within_day, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.dtypes\n",
    "# Y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "precision_recall_fscore_support(Y_test, clf.predict(X_test), average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see results notebook for removing C = 1000 for the moment\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100], 'kernel': ['linear'], 'class_weight': ['balanced'] },\n",
    "  {'C': [1, 10, 100], 'gamma': [0.001, 0.0001], 'kernel': ['rbf'], 'class_weight': ['balanced'] },\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_svc = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = GridSearchCV(search_svc, param_grid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.fit(X_encoded, data.is_save_within_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we should really separate out test prior to train, with just a little more data\n",
    "scores = precision_recall_fscore_support(Y_test, svc_clf.predict(X_test), average='binary')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, svc_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_dict(svc_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARTING XBOOST SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier(min_samples_split=10)\n",
    "gb_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(Y_test, gb_clf.predict(X_test), average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERSIST THE MODEL AND UPLOAD IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_model(clf, send_to_storage=False, storage_bucket=None, latest_model_name=None):\n",
    "    file_name = f\"boost_incuding_model_{datetime.today().strftime('%Y_%m_%dT%H:%M:%S')}.joblib\"\n",
    "    dump(clf, file_name)\n",
    "    print(f\"Model dumped to {file_name}\")\n",
    "    \n",
    "    if send_to_storage:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(storage_bucket)\n",
    "        blob = bucket.blob(file_name)\n",
    "\n",
    "        blob.upload_from_filename(f\"./{file_name}\")\n",
    "        print(f\"File {file_name} uploaded to {storage_bucket}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_model(svc_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_clf = load('boost_incuding_model_2020_06_23T18:16:43.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, restored_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STORE RESULTS IN DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_client = datastore.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = \"TrainingResult\"\n",
    "name = f\"boost_inducement_{datetime.today().strftime('%Y_%m_%dT%H:%M:%S')}\"\n",
    "result_key = datastore_client.key(kind, name)\n",
    "print('Result key: ', result_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = datastore.Entity(key=result_key)\n",
    "# model_result['recall'] = results['recall']\n",
    "# model_result['precision'] = results['precision']\n",
    "# model_result['accuracy'] = results['accuracy']\n",
    "# model_result['n'] = result_store['n']\n",
    "# model_result['n_positive'] = result_store['n_positive']\n",
    "\n",
    "# model_result['description'] = 'This is a bit lousy'\n",
    "\n",
    "model_result.update(result_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_client.put(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupiter-ml] *",
   "language": "python",
   "name": "conda-env-jupiter-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
