{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dabl\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_monthly_save_aggregates():\n",
    "    sql = \"\"\"\n",
    "        select EXTRACT(MONTH from TIMESTAMP_MILLIS(time_transaction_occurred)) as save_month, unit, \n",
    "            sum(amount) as sum, avg(amount) as average, count(*) as count from ops.user_behaviour \n",
    "            where transaction_type = 'SAVING_EVENT' group by save_month, unit order by save_month desc;\n",
    "    \"\"\"\n",
    "    \n",
    "    df = client.query(sql).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually just use a pandas mask to split forward and back\n",
    "def obtain_boosts_with_saves():\n",
    "    sql = \"\"\"\n",
    "        with boost_offers as (\n",
    "            select *, TIMESTAMP_MILLIS(created_at) as creation_timestamp \n",
    "            from ops.all_user_events \n",
    "            where event_type like 'BOOST_CREATED%'\n",
    "\n",
    "    ), save_events as (\n",
    "            select *, TIMESTAMP_MILLIS(created_at) as creation_timestamp \n",
    "            from ops.all_user_events \n",
    "            where event_type = 'SAVING_PAYMENT_SUCCESSFUL'\n",
    "    )\n",
    "    select boost_offers.user_id, boost_offers.event_type, boost_offers.context, \n",
    "        boost_offers.creation_timestamp as boost_creation_time, save_events.creation_timestamp as save_completion_time,  \n",
    "        TIMESTAMP_DIFF(save_events.creation_timestamp, boost_offers.creation_timestamp, HOUR) as time_from_boost_to_save\n",
    "    from boost_offers left join save_events on boost_offers.user_id = save_events.user_id\n",
    "    \"\"\"\n",
    "    \n",
    "    df = client.query(sql).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prior_save_counts(prior_save_counts):\n",
    "    print('Past rows: ', prior_save_counts.shape)\n",
    "    prior_save_counts[\"boost_prior_saves\"] = prior_save_counts.groupby('boost_user_id').transform('count')[\"save_completion_time\"]\n",
    "    prior_save_counts = prior_save_counts[[\"boost_user_id\", \"boost_prior_saves\"]]\n",
    "    prior_save_counts = prior_save_counts.groupby(\"boost_user_id\").first() # no need for a sort\n",
    "    return prior_save_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_since_latest_save(prior_save_df):\n",
    "    with_latest_save = prior_save_df.sort_values(\"save_completion_time\").groupby(\"boost_user_id\", as_index = False).last()\n",
    "    with_latest_save[\"days_since_latest_save\"] = abs(with_latest_save[\"time_from_boost_to_save\"] / 24)\n",
    "    with_latest_save = with_latest_save[[\"boost_user_id\", \"days_since_latest_save\"]]\n",
    "    return with_latest_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_since_first_save(prior_save_df):\n",
    "    # for some reason if index, causes issues here\n",
    "    with_earliest_save = prior_save_df.sort_values(\"save_completion_time\").groupby(\"boost_user_id\", as_index = False).first()\n",
    "    with_earliest_save[\"days_since_first_save\"] = abs(with_earliest_save[\"time_from_boost_to_save\"] / 24)\n",
    "    with_earliest_save = with_earliest_save[[\"boost_user_id\", \"days_since_first_save\"]]\n",
    "    return with_earliest_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_and_construct_labels(boosts_with_saves):\n",
    "    unit_convertors = { 'WHOLE_CURRENCY': 1, 'WHOLE_CENT': 100, 'HUNDREDTH_CENT': 10000 }\n",
    "    \n",
    "    df = boosts_with_saves\n",
    "    print('Starting count: ', df.shape)\n",
    "    \n",
    "    df['user_id_count'] = boosts_with_saves.groupby(['user_id'])['boost_creation_time'].transform('count')\n",
    "    \n",
    "    # we remove the top 2, because they are team members often testing, so distort\n",
    "    outlier_user_ids = df['user_id'].value_counts()[:2].index.tolist()\n",
    "    # probably a better panda-ninja way to do this but not worth it right now\n",
    "    for user_id in outlier_user_ids:\n",
    "        df = df[df.user_id != user_id]\n",
    "        \n",
    "    print('With outlier top users stripped: ', df.shape)\n",
    "    \n",
    "    # here we have our label\n",
    "    df[\"is_save_within_day\"] = df[\"time_from_boost_to_save\"] < 24\n",
    "    \n",
    "    # extract a bunch of context from the boosts    \n",
    "    df[\"parsed_context\"] = df.context.apply(json.loads)\n",
    "    df[\"boost_id\"] = df[\"parsed_context\"].apply(lambda context: context[\"boostId\"])\n",
    "    df[\"boost_amount_whole_currency\"] = df[\"parsed_context\"].apply(\n",
    "        lambda context: context[\"boostAmount\"] / unit_convertors[context[\"boostUnit\"]])\n",
    "    \n",
    "    df[\"boost_type\"] = df[\"parsed_context\"].apply(lambda context: context[\"boostType\"])\n",
    "    df[\"boost_category\"] = df[\"parsed_context\"].apply(lambda context: context[\"boostCategory\"])\n",
    "    df[\"boost_type_category\"] = df[\"boost_type\"] + \"::\" + df[\"boost_category\"]\n",
    "    \n",
    "    df[\"day_of_month\"] = df[\"boost_creation_time\"].dt.day\n",
    "    df[\"hour_of_day\"] = df[\"boost_creation_time\"].dt.hour\n",
    "    \n",
    "    # and this functions as our index     \n",
    "    df[\"boost_user_id\"] = df[\"boost_id\"] + \"::\" + df[\"user_id\"]\n",
    "    \n",
    "    # then we construct our future and past masks, calculate prior saves, and find next save\n",
    "    prior_save_mask = df[\"time_from_boost_to_save\"] < 0\n",
    "    future_save_mask = df[\"time_from_boost_to_save\"] > 0\n",
    "        \n",
    "    # likely a way to do these more simply, but for now doing groups & sorts differently    \n",
    "    prior_save_counts = extract_prior_save_counts(df[prior_save_mask].copy())\n",
    "    days_since_latest_save = extract_time_since_latest_save(df[prior_save_mask].copy())\n",
    "    days_since_first_save = extract_time_since_first_save(df[prior_save_mask].copy())\n",
    "    \n",
    "    # then we discard the past\n",
    "    with_future_saves = df[future_save_mask].copy()\n",
    "    with_next_save = with_future_saves.sort_values(\"save_completion_time\").groupby(\"boost_user_id\").first()\n",
    "    \n",
    "    print('Now with just future saves crossed: ', with_future_saves.shape, ' and next save only: ', with_next_save.shape)\n",
    "    \n",
    "    # and finally we strip out the surplus boost-save pairs (by retaining only the opening)\n",
    "    # at the moment an inner join, but we may want to turn this into joining from those with saves\n",
    "    final_df = pd.merge(with_next_save, prior_save_counts, on='boost_user_id')\n",
    "    final_df = pd.merge(final_df, days_since_latest_save, on='boost_user_id')\n",
    "    final_df = pd.merge(final_df, days_since_first_save, on='boost_user_id')\n",
    "    print(\"And finally, stripped to just one per: \", final_df.shape)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data):\n",
    "    features_of_interest = [\n",
    "        \"boost_amount_whole_currency\", \n",
    "        \"day_of_month\", \n",
    "        \"boost_prior_saves\",\n",
    "        \"boost_type_category\",\n",
    "        \"days_since_latest_save\",\n",
    "        \"days_since_first_save\",\n",
    "        \"is_save_within_day\"\n",
    "    ]\n",
    "    stripped_df = data[features_of_interest]\n",
    "    return stripped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosts_with_saves = obtain_boosts_with_saves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_up_and_construct_labels(boosts_with_saves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_save_within_day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boost_prior_saves.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_frame = feature_extraction(data)\n",
    "feature_frame.dtypes\n",
    "# feature_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dabl_data = dabl.clean(feature_frame)\n",
    "dabl_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dabl.plot(dabl_data, target_col='is_save_within_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dabl_data.drop(\"is_save_within_day\", axis=1)\n",
    "Y = dabl_data.is_save_within_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment this is making things worse, so\n",
    "# preprocessor = dabl.EasyPreprocessor()\n",
    "# X_trans = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = dabl.SimpleClassifier(random_state=0)\n",
    "fc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X, prefix_sep=\"_\", columns=[\"boost_type_category\"])\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_encoded, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC()\n",
    "# cross_val_score(clf, X_encoded, Y, cv=5, scoring='recall_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.value_counts()\n",
    "# Y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(Y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupiter-ml] *",
   "language": "python",
   "name": "conda-env-jupiter-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
